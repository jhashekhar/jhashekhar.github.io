<!DOCTYPE html>
<html lang="en-US">
  <head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <meta charset="UTF-8">


<style>
* {
  box-sizing: border-box;
}


/* Create two unequal columns that floats next to each other */
.column {
  float: right;
  padding: 10px;
}

.left {
  width: 0%;
  position: fixed;
  text-align: left;
}

.right {
  width: 100%;
  padding-left: 20px;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}

.row li {
  margin: 0 0 4px 0;
}
</style>


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Limitations of Landscape Approach for understanding Optimization | Shekhar Jha</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Limitations of Landscape Approach for understanding Optimization" />
<meta name="author" content="Shekhar Jha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Limitations of Landscape Approach for understanding Optimization Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN). The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than 3 dimensional surface. Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an over-parametrized setting. As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence. First, no poor local minima - local minimum is as good as global minimum. Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum. There is at least one direction along which the curvature is negative and that is required for us to escape saddle points. Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers. So we need to take a step back and look out for other approaches that could help us better understand optimization." />
<meta property="og:description" content="Limitations of Landscape Approach for understanding Optimization Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN). The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than 3 dimensional surface. Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an over-parametrized setting. As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence. First, no poor local minima - local minimum is as good as global minimum. Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum. There is at least one direction along which the curvature is negative and that is required for us to escape saddle points. Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers. So we need to take a step back and look out for other approaches that could help us better understand optimization." />
<link rel="canonical" href="http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html" />
<meta property="og:url" content="http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html" />
<meta property="og:site_name" content="Shekhar Jha" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-01T00:00:00+05:30" />
<script type="application/ld+json">
{"headline":"Limitations of Landscape Approach for understanding Optimization","dateModified":"2019-01-01T00:00:00+05:30","datePublished":"2019-01-01T00:00:00+05:30","url":"http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html"},"author":{"@type":"Person","name":"Shekhar Jha"},"description":"Limitations of Landscape Approach for understanding Optimization Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN). The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than 3 dimensional surface. Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an over-parametrized setting. As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence. First, no poor local minima - local minimum is as good as global minimum. Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum. There is at least one direction along which the curvature is negative and that is required for us to escape saddle points. Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers. So we need to take a step back and look out for other approaches that could help us better understand optimization.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=62a8776149a8c776c2d50f3d36dffe6e291423a7">
  </head>

  <body style="max-width: 900px;">
      <div class="row">
          <div class="column right">
                <main id="content" class="main-content" role="main">
                  <p style="text-align:right;"><a href="https://jhashekhar.github.io"><b>Home</b></a></p>
                  <hr><br>
                    <title-head><h1>Limitations of Landscape Approach for understanding Optimization</h1></title-head>
<p><br />
Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN).</p>

<p>The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with
this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than
3 dimensional surface.</p>

<p>Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an
over-parametrized setting.</p>

<p>As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence.</p>

<p><em>First, no poor local minima - local minimum is as good as global minimum.</em></p>

<p><em>Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum.</em></p>

<p>There is at least one direction along which the curvature is negative and that is required for us to escape saddle points.</p>

<p>Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers.</p>

<p>So we need to take a step back and look out for other approaches that could help us better understand optimization.</p>

                </main>
          </div>
      </div>
    </body>
</html>
