<!DOCTYPE html>
<html lang="en-US">
  <head>

    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Limitations of Landscape Approach for understanding Optimization | Shekhar’s Blog</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Limitations of Landscape Approach for understanding Optimization" />
<meta name="author" content="Shekhar Jha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Limitations of Landscape Approach for understanding Optimization Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN). The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than 3 dimensional surface. Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an over-parametrized setting. As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence. First, no poor local minima - local minimum is as good as global minimum. Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum. There is at least one direction along which the curvature is negative and that is required for us to escape saddle points. Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers. So we need to take a step back and look out for other approaches that could help us better understand optimization." />
<meta property="og:description" content="Limitations of Landscape Approach for understanding Optimization Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN). The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than 3 dimensional surface. Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an over-parametrized setting. As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence. First, no poor local minima - local minimum is as good as global minimum. Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum. There is at least one direction along which the curvature is negative and that is required for us to escape saddle points. Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers. So we need to take a step back and look out for other approaches that could help us better understand optimization." />
<link rel="canonical" href="http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html" />
<meta property="og:url" content="http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html" />
<meta property="og:site_name" content="Shekhar’s Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-01-01T00:00:00+05:30" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Shekhar Jha"},"headline":"Limitations of Landscape Approach for understanding Optimization","dateModified":"2019-01-01T00:00:00+05:30","datePublished":"2019-01-01T00:00:00+05:30","description":"Limitations of Landscape Approach for understanding Optimization Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN). The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than 3 dimensional surface. Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an over-parametrized setting. As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence. First, no poor local minima - local minimum is as good as global minimum. Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum. There is at least one direction along which the curvature is negative and that is required for us to escape saddle points. Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers. So we need to take a step back and look out for other approaches that could help us better understand optimization.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html"},"@type":"BlogPosting","url":"http://localhost:4000/2019/01/01/limitations-of-landscape-approach.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=b9693abd96ae1df19505841fa7399837db4ea0a5">
  </head>
  <body>
  <header class="neo-header">
    <left-item><a href="../../../blog.html">Blog</a></left-item>
    <right-item><a href="../../../index.html">Home</a></right-item>
    <hr>
  </header>
    <main id="content" class="main-content" role="main">
      <title-head><h2><u>Limitations of Landscape Approach for understanding Optimization</u></h2></title-head>
<p><br />
Here I will write briefly about different approaches towards convergence analysis in the case of Deep Neural Networks(DNN).</p>

<p>The first approach that has been taken by most researchers is the analysis of loss surface(landscape). The problem with
this approach is that the loss surface is multi-dimensional(millions of parameters) and we cannot visualize greater than
3 dimensional surface.</p>

<p>Another problem is the introduction of non-linearity in DNN. It’s very hard to understand the effect of non-linearity over such an
over-parametrized setting.</p>

<p>As it has been pointed out in Arora et al. 2019 that loss surface analysis has two main conditions that should be met for convergence.</p>

<p><em>First, no poor local minima - local minimum is as good as global minimum.</em></p>

<p><em>Second, strict saddle property - the hessian has at least one negative eigenvalue for critical points other than the local minimum.</em></p>

<p>There is at least one direction along which the curvature is negative and that is required for us to escape saddle points.</p>

<p>Both of the above mentioned conditions that are considered to be sufficient to have convergence to global minimum are proved to not hold by different papers.</p>

<p>So we need to take a step back and look out for other approaches that could help us better understand optimization.</p>

    </main>
  </body>
</html>