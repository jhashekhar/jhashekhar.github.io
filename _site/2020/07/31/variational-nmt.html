<!DOCTYPE html>
<html lang="en-US">
  <head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Variational Neural Machine Translation | Shekhar Jha</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Variational Neural Machine Translation" />
<meta name="author" content="Shekhar Jha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Variational Neural Machine Translation While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled Variational Neural Machine Translation with Normalizing Flows. I&#39;ve been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT. So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT. Addressing the translational ambiguity Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ? Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities. To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables: Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy. It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large). Variational NMT (VNMT) belongs to this family of LVNMT. Now I&#39;ve introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let&#39;s go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now: tldr: Machine translation has ambiguities - depending upon author&#39;s style, context, domain, etc. Current NMT models gives SOTA results but doesn&#39;t have explicit mechanisms to address these ambiguities. To address it, Latent Variable Model NMT (LVNMT) is introduced. LVNMT models have intractable posterior and to get around that strong assumptions are imposed. Variational NMT with Normalizing Flows tries to address these issues. Latent Variable Models Latent variable models defines distribution over a given sample x by using a latent variable \(z\). Most often \(z\) is a vector. latent variable \(z\) is sampled from a prior distribution \(p(z)\) - typically Gaussian distribution. \(p(x|z)\) is the likelihood of the sample. All these ideas work for parameterized distribtuion as well. Variational Inference" />
<meta property="og:description" content="Variational Neural Machine Translation While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled Variational Neural Machine Translation with Normalizing Flows. I&#39;ve been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT. So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT. Addressing the translational ambiguity Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ? Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities. To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables: Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy. It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large). Variational NMT (VNMT) belongs to this family of LVNMT. Now I&#39;ve introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let&#39;s go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now: tldr: Machine translation has ambiguities - depending upon author&#39;s style, context, domain, etc. Current NMT models gives SOTA results but doesn&#39;t have explicit mechanisms to address these ambiguities. To address it, Latent Variable Model NMT (LVNMT) is introduced. LVNMT models have intractable posterior and to get around that strong assumptions are imposed. Variational NMT with Normalizing Flows tries to address these issues. Latent Variable Models Latent variable models defines distribution over a given sample x by using a latent variable \(z\). Most often \(z\) is a vector. latent variable \(z\) is sampled from a prior distribution \(p(z)\) - typically Gaussian distribution. \(p(x|z)\) is the likelihood of the sample. All these ideas work for parameterized distribtuion as well. Variational Inference" />
<link rel="canonical" href="http://localhost:4000/2020/07/31/variational-nmt.html" />
<meta property="og:url" content="http://localhost:4000/2020/07/31/variational-nmt.html" />
<meta property="og:site_name" content="Shekhar Jha" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-31T00:00:00+05:30" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"Shekhar Jha"},"headline":"Variational Neural Machine Translation","dateModified":"2020-07-31T00:00:00+05:30","description":"Variational Neural Machine Translation While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled Variational Neural Machine Translation with Normalizing Flows. I&#39;ve been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT. So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT. Addressing the translational ambiguity Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ? Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities. To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables: Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy. It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large). Variational NMT (VNMT) belongs to this family of LVNMT. Now I&#39;ve introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let&#39;s go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now: tldr: Machine translation has ambiguities - depending upon author&#39;s style, context, domain, etc. Current NMT models gives SOTA results but doesn&#39;t have explicit mechanisms to address these ambiguities. To address it, Latent Variable Model NMT (LVNMT) is introduced. LVNMT models have intractable posterior and to get around that strong assumptions are imposed. Variational NMT with Normalizing Flows tries to address these issues. Latent Variable Models Latent variable models defines distribution over a given sample x by using a latent variable \\(z\\). Most often \\(z\\) is a vector. latent variable \\(z\\) is sampled from a prior distribution \\(p(z)\\) - typically Gaussian distribution. \\(p(x|z)\\) is the likelihood of the sample. All these ideas work for parameterized distribtuion as well. Variational Inference","datePublished":"2020-07-31T00:00:00+05:30","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/07/31/variational-nmt.html"},"@type":"BlogPosting","url":"http://localhost:4000/2020/07/31/variational-nmt.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=56730e6f2f73e6f838bf5d249edf005812bc2502">
  </head>
  <body style="max-width: 1200px;">
  <header class="neo-header">
    <right-item><a href="../../../index.html"> << Back to Homepage</a></right-item>
  </header>
    <main id="content" class="main-content" role="main">
      <title-head><h2><u>Variational Neural Machine Translation</u></h2></title-head>
<p><br /></p>
<p>While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled <a href="">Variational Neural Machine Translation with Normalizing Flows</a>. I've been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT.</p>

<p> So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT.</p>

<h3><i>Addressing the translational ambiguity</i></h3>
<hr />

<p>Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ?</p>

<p>Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities.</p>

<p>To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables:</p>

<ul style="font-size: 15px;">
<li>Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy.</li>
<br />
<li>It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large).</li>
</ul>

<p>Variational NMT (VNMT) belongs to this family of LVNMT.</p>
<p> Now I've introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let's go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now:</p>

<h3>tldr:</h3>
<hr />

<ul style="font-size: 15px;">
<li>Machine translation has ambiguities - depending upon author's style, context, domain, etc.</li>
<li>Current NMT models gives SOTA results but doesn't have explicit mechanisms to address these ambiguities.</li>
<li>To address it, Latent Variable Model NMT (LVNMT) is introduced.</li>
<li>LVNMT models have intractable posterior and to get around that strong assumptions are imposed.</li>
<li>Variational NMT with Normalizing Flows tries to address these issues.</li>
</ul>
<p><br /></p>

<h3><i>Latent Variable Models</i></h3>
<hr />

<p>Latent variable models defines distribution over a given sample x by using a latent variable \(z\). Most often \(z\) is a vector.</p>

<ul style="font-size: 15px;">
<li>latent variable \(z\) is sampled from a prior distribution \(p(z)\) - typically Gaussian distribution.</li>
<li>\(p(x|z)\) is the likelihood of the sample.</li>
<li>All these ideas work for parameterized distribtuion as well.</li>
</ul>

<h3><i></i></h3>
<h3><i>Variational Inference</i></h3>


    </main>
  </body>
</html>