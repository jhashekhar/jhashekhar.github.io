<!DOCTYPE html>
<html lang="en-US">
  <head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <meta charset="UTF-8">


<style>
* {
  box-sizing: border-box;
}


/* Create two unequal columns that floats next to each other */
.column {
  float: right;
  padding: 10px;
}

.left {
  width: 0%;
  position: fixed;
  text-align: left;
}

.right {
  width: 100%;
  padding-left: 40px;
}

/* Clear floats after the columns */
.row:after {
  content: "";
  display: table;
  clear: both;
}

.row li {
  margin: 0 0 4px 0;
}

img {
 display: block;
 height: 100%;
 width: 100%
}
</style>


<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Variational NMT | Shekhar Jha</title>
<meta name="generator" content="Jekyll v3.8.7" />
<meta property="og:title" content="Variational NMT" />
<meta name="author" content="Shekhar Jha" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Variational Neural Machine Translation While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled Variational Neural Machine Translation with Normalizing Flows. I&#39;ve been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT. So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT. Addressing the translational ambiguity Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ? Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities. To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables: Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy. It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large). Variational NMT (VNMT) belongs to this family of LVNMT. Now I&#39;ve introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let&#39;s go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now: Quick recap: Machine translation has ambiguities - depending upon author&#39;s style, context, domain, etc. Current NMT models gives SOTA results but doesn&#39;t have explicit mechanisms to address these ambiguities. To address it, Latent Variable Model NMT (LVNMT) is introduced. LVNMT models have intractable posterior and to get around that strong assumptions are imposed. Variational NMT with Normalizing Flows tries to address these issues. Latent Variable Models Latent variable models defines distribution over a given sample x by using a latent variable \(z\). Most often \(z\) is a vector. Latent variable \(z\) is sampled from a prior distribution \(p(z)\). Prior distribution \(p(z)\) is typically Gaussian distribution. \(p(x|z)\) is the likelihood of the sample. All these ideas work for parameterized distribtuion as well. There are several ways to understand the intuition behind latent variables, the one that sticks with me is given by Carl Doersch - MNIST digit generation. To generate a digit the generator/decoder first needs to pick the digit before populating the pixels containg strokes, boundaries etc. The decoder while having drawn half-way through 5 cannot decide on drawing 0 as that would resemble none of the digits. This kind of decision to select the digit before starting the drawing is called latent variable. Also, latent variable in most cases is a vector and given the digit that&#39;s been generated, it is hard to know which &quot;latent&quot; vector lead to it and that is the reason for calling the vector latent. $$p_\theta(x, z) = p(z|x) p(x) = p(x|z) p(z)$$ Variational Inference In variational inference we try to minimize the Evidence Lower Bound (ELBO): $$L_{\theta, \phi}(x) = \log p_\theta(x) - KL(q_\phi(z|x) || p_\theta(z|x))$$ where, \(\log p_\theta(x)\) is reconstruction error and second term is the KL Divergence. Todo : Expand on Variational Inference - especially the intuitive interpretation of the ELBO. Adapt ELBO for VNMT and need to understand different kind of Flows - their strengths and limitations." />
<meta property="og:description" content="Variational Neural Machine Translation While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled Variational Neural Machine Translation with Normalizing Flows. I&#39;ve been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT. So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT. Addressing the translational ambiguity Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ? Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities. To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables: Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy. It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large). Variational NMT (VNMT) belongs to this family of LVNMT. Now I&#39;ve introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let&#39;s go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now: Quick recap: Machine translation has ambiguities - depending upon author&#39;s style, context, domain, etc. Current NMT models gives SOTA results but doesn&#39;t have explicit mechanisms to address these ambiguities. To address it, Latent Variable Model NMT (LVNMT) is introduced. LVNMT models have intractable posterior and to get around that strong assumptions are imposed. Variational NMT with Normalizing Flows tries to address these issues. Latent Variable Models Latent variable models defines distribution over a given sample x by using a latent variable \(z\). Most often \(z\) is a vector. Latent variable \(z\) is sampled from a prior distribution \(p(z)\). Prior distribution \(p(z)\) is typically Gaussian distribution. \(p(x|z)\) is the likelihood of the sample. All these ideas work for parameterized distribtuion as well. There are several ways to understand the intuition behind latent variables, the one that sticks with me is given by Carl Doersch - MNIST digit generation. To generate a digit the generator/decoder first needs to pick the digit before populating the pixels containg strokes, boundaries etc. The decoder while having drawn half-way through 5 cannot decide on drawing 0 as that would resemble none of the digits. This kind of decision to select the digit before starting the drawing is called latent variable. Also, latent variable in most cases is a vector and given the digit that&#39;s been generated, it is hard to know which &quot;latent&quot; vector lead to it and that is the reason for calling the vector latent. $$p_\theta(x, z) = p(z|x) p(x) = p(x|z) p(z)$$ Variational Inference In variational inference we try to minimize the Evidence Lower Bound (ELBO): $$L_{\theta, \phi}(x) = \log p_\theta(x) - KL(q_\phi(z|x) || p_\theta(z|x))$$ where, \(\log p_\theta(x)\) is reconstruction error and second term is the KL Divergence. Todo : Expand on Variational Inference - especially the intuitive interpretation of the ELBO. Adapt ELBO for VNMT and need to understand different kind of Flows - their strengths and limitations." />
<link rel="canonical" href="http://localhost:4000/2020/07/31/vnmt.html" />
<meta property="og:url" content="http://localhost:4000/2020/07/31/vnmt.html" />
<meta property="og:site_name" content="Shekhar Jha" />
<meta property="og:image" content="https://jhashekhar.github.io/assets/img/lvm/latent-random-variable.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-31T00:00:00+05:30" />
<script type="application/ld+json">
{"image":"https://jhashekhar.github.io/assets/img/lvm/latent-random-variable.png","headline":"Variational NMT","dateModified":"2020-07-31T00:00:00+05:30","datePublished":"2020-07-31T00:00:00+05:30","url":"http://localhost:4000/2020/07/31/vnmt.html","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2020/07/31/vnmt.html"},"author":{"@type":"Person","name":"Shekhar Jha"},"description":"Variational Neural Machine Translation While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled Variational Neural Machine Translation with Normalizing Flows. I&#39;ve been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT. So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT. Addressing the translational ambiguity Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ? Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities. To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables: Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy. It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large). Variational NMT (VNMT) belongs to this family of LVNMT. Now I&#39;ve introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let&#39;s go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now: Quick recap: Machine translation has ambiguities - depending upon author&#39;s style, context, domain, etc. Current NMT models gives SOTA results but doesn&#39;t have explicit mechanisms to address these ambiguities. To address it, Latent Variable Model NMT (LVNMT) is introduced. LVNMT models have intractable posterior and to get around that strong assumptions are imposed. Variational NMT with Normalizing Flows tries to address these issues. Latent Variable Models Latent variable models defines distribution over a given sample x by using a latent variable \\(z\\). Most often \\(z\\) is a vector. Latent variable \\(z\\) is sampled from a prior distribution \\(p(z)\\). Prior distribution \\(p(z)\\) is typically Gaussian distribution. \\(p(x|z)\\) is the likelihood of the sample. All these ideas work for parameterized distribtuion as well. There are several ways to understand the intuition behind latent variables, the one that sticks with me is given by Carl Doersch - MNIST digit generation. To generate a digit the generator/decoder first needs to pick the digit before populating the pixels containg strokes, boundaries etc. The decoder while having drawn half-way through 5 cannot decide on drawing 0 as that would resemble none of the digits. This kind of decision to select the digit before starting the drawing is called latent variable. Also, latent variable in most cases is a vector and given the digit that&#39;s been generated, it is hard to know which &quot;latent&quot; vector lead to it and that is the reason for calling the vector latent. $$p_\\theta(x, z) = p(z|x) p(x) = p(x|z) p(z)$$ Variational Inference In variational inference we try to minimize the Evidence Lower Bound (ELBO): $$L_{\\theta, \\phi}(x) = \\log p_\\theta(x) - KL(q_\\phi(z|x) || p_\\theta(z|x))$$ where, \\(\\log p_\\theta(x)\\) is reconstruction error and second term is the KL Divergence. Todo : Expand on Variational Inference - especially the intuitive interpretation of the ELBO. Adapt ELBO for VNMT and need to understand different kind of Flows - their strengths and limitations.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=3e718288896142a572951f366fe12aa6fffe6cb1">
  </head>

  <body style="max-width: 800px;">
      <div class="row">
          <div class="column right">
                <main id="content" class="main-content" role="main">
                    <title-head><h2>Variational Neural Machine Translation</h2></title-head>

<p>While going deeper into Speech synthesis and Machine Translation, specifically Neural Machine Translation. I encountered this great paper titled <a href="https://arxiv.org/abs/2005.13978">Variational Neural Machine Translation with Normalizing Flows</a>. I've been familiar with Variational part of the title and somewhat rusty understanding of Normalizing Flow part. I was really excited with these frameworks application in NMT.</p>

<p> So, given the situation I would try to come up with an intuitive understanding of the Variational Inference, Normalizing Flows and different sub topics within these two topics from the perspective of NMT.</p>

<p><br /></p>

<h4><b>Addressing the translational ambiguity</b></h4>
<p>Translation is inherently ambiguous. It is dependent on the context, domain and other factors. So we need to have a translation model that accounts for this ambiguity. So, how the current models addresses this ambiguity and how can we improve upon them ?</p>

<p>Currently NMT is used widely and it has been constantly churning out state-of-the-art results since its introduction. But the NMT framework has no explicit mechanisms to account for translation amboguities.</p>

<p>To address the issues mentioned above Latent Variable NMT(LVNMT) was introduced. There are few things that we need to keep in mind about Latent variables:</p>

<ul>
    <li>Analyzing the latent variables is difficult and tricky. Think of trying to infer the state of learning while analyzing the attention values or the values of embedding vecotrs in Word2Vec or the in CV analysing the saliency maps. Experiments shows that latent codes improve accuracy.</li>
    
    <li>It adds complexity to the model that makes the model flexible but also leads to the problem of intractable inference.(The intergration does not have analytic solution and numerical computation is very large).</li>
</ul>

<p>Variational NMT (VNMT) belongs to this family of LVNMT.</p>
<p> Now I've introduced lot of keywords; Latent Variables, Latent Variable Model, Intractable inference, Variational NMT. Let's go through these and try to develop intuition regarding these. Before that a quick summary of what we learnt till now:</p>

<p><br /></p>

<h4><b>Quick recap:</b></h4>
<p>
<ul>
    <li>Machine translation has ambiguities - depending upon author's style, context, domain, etc.</li>
    <li>Current NMT models gives SOTA results but doesn't have explicit mechanisms to address these ambiguities.</li>
    <li>To address it, Latent Variable Model NMT (LVNMT) is introduced.</li>
    <li>LVNMT models have intractable posterior and to get around that strong assumptions are imposed.</li>
    <li>Variational NMT with Normalizing Flows tries to address these issues.</li>
</ul>
</p>
<p><br /></p>

<h4><b>Latent Variable Models</b></h4>
<table>
        <td>
            <img src="https://jhashekhar.github.io/assets/img/lvm/latent-random-variable.png" />
        </td>
        <td>
            <p>Latent variable models defines distribution over a given sample x by using a latent variable \(z\). Most often \(z\) is a vector.</p>
                <ul>
                    <li>Latent variable \(z\) is sampled from a prior distribution \(p(z)\).</li>
                    <li><b>Prior distribution \(p(z)\)</b> is typically Gaussian distribution.</li>
                    <li><b>\(p(x|z)\) is the likelihood</b> of the sample.</li>
                    <li>All these ideas work for parameterized distribtuion as well.</li>
            </ul>
        </td>
</table>

<p>There are several ways to understand the intuition behind latent variables, the one that sticks with me is given by Carl Doersch - MNIST digit generation. To generate a digit the generator/decoder first needs to pick the digit before populating the pixels containg strokes, boundaries etc. The decoder while having drawn half-way through 5 cannot decide on drawing 0 as that would resemble none of the digits. This kind of decision to select the digit before starting the drawing is called latent variable. Also, latent variable in most cases is a vector and given the digit that's been generated, it is hard to know which "latent" vector lead to it and that is the reason for calling the vector latent.</p>

<p style="font-size: 16px;"><b>$$p_\theta(x, z) = p(z|x) p(x) = p(x|z) p(z)$$</b></p>

<h4><b>Variational Inference</b></h4>

<p>In variational inference we try to minimize the Evidence Lower Bound (ELBO):</p>

<p style="font-size: 16px;"><b>$$L_{\theta, \phi}(x) = \log p_\theta(x) - KL(q_\phi(z|x) || p_\theta(z|x))$$</b></p>

<p>where, <b>\(\log p_\theta(x)\)</b> is reconstruction error and second term is the KL Divergence.</p>

<p><br /></p>

<h4><b>Todo :</b></h4>
<p>
<ul>
<li> Expand on Variational Inference - especially the intuitive interpretation of the ELBO.</li>
<li> Adapt ELBO for VNMT and need to understand different kind of Flows - their strengths and limitations.</li>
</ul>
</p>

                </main>
          </div>
      </div>
    </body>
</html>
