---
layout: post
author: Shekhar Jha
title: "Optimization: Few Questions"
tags: [deep learning, optimization, generalization, gradient-descent]
excerpt_separator: <!--more-->
date: 2018-11-12
---
<title-head><h2>{{ page.title }}</h2></title-head>

<p>
After reading and doing some toy projects in deep learning I got interested in various aspects of non-convex optimization,
expressiveness of deep neural nets, generalization to name a few. <!--more-->In this post I would be listing down some of the questions that came to my mind while studying and doing problems in no specific order.</p>

<ul>
<li>Why gradient descent works ?</li>
<li>How does depth of neural network helps ?</li>
<li>Can a very large width network have same function approximation power as a significantly deeper network ?</li>
<li>What ensures generalization of empirical hypotheses ?</li>
<li>Depth improves expressiveness but complicates optimization - Why and How ?</li>
</ul>
