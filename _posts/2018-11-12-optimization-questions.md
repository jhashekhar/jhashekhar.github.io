---
layout: post
author: Shekhar Jha
title: "Optimization: Some Questions"
tags: [deep learning, optimization, generalization, gradient-descent]
excerpt_separator: <!--more-->
date: 2018-11-12
---

After reading and doing some toy projects in deep learning I got interested in various aspects of non-convex optimization,
expressiveness of deep neural nets, generalization to name a few. <!--more-->In this post I would be listing down some of the questions that came to my mind while studying and doing problems in no specific order.

* Why gradient descent works ?
* How does depth of neural network helps ?
* Can a very large width network have same function approximation power as a significantly deeper network ?
* What ensures generalization of empirical hypotheses ?
* Depth improves expressiveness but complicates optimization - Why and How ?
